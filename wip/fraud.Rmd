---
title: "Autoencoders and anomaly detection with machine learning in fraud analytics"
author: "Dr. Shirin Glander"
date: "April 24, 2017"
output: html_document
---

All my previous posts on machine learning have dealt with supervised learning. But we can also use machine learning for unsupervised learning. The latter are used for clustering and (non-linear) dimensionality reduction.

For this task, I am using [Kaggle's creditcard fraud dataset](https://www.kaggle.com/dalpozz/creditcardfraud) from the following study: 

> Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015

The dataset gives > 280,000 instances of creditcard use, and for each transaction, we know whether it was fraudulent or not.

Data like this needs special treatment when performing machine learning because they are severely unbalanced: in this case, only 0.17% of all transactions are fraudulent.

While we could try to work with classifiers, like random forests or support vector machines, by applying [over- or under-sampling techniques](https://shiring.github.io/machine_learning/2017/04/02/unbalanced), we can alternatively try to find anomalies in the data (assuming we expect our fraud cases to be anomalies within the whole dataset).

When dealing with such a severe unbalance of response labels, we also need to careful when measuring model performance. Because there are only a handful of fraudulent instances, a model that predicts everything as non-fraud will already achieve a > 99% accuracy. But we won't be able to find fraudulent cases with it - the task we actually want to achieve!

Below, I will show how you could use autoencoders and anomaly detection, how you could use autoencoders to pre-train a classification model and how you could measure model performance on unbalanced data.

<br>

## Exploring the data

```{r message=FALSE, warning=FALSE}
library(tidyverse)
```

```{r eval=FALSE}
# download from https://www.kaggle.com/dalpozz/creditcardfraud
creditcard <- read.csv("~/Documents/Github/creditcard/creditcard.csv")
```

```{r echo=FALSE, eval=FALSE}
save(creditcard, file = "~/Documents/Github/creditcard/creditcard.RData")
```

```{r echo=FALSE}
load("~/Documents/Github/creditcard/creditcard.RData")
```

The dataset contains numerical input variables V1 to V28, which are the result of a PCA transformation of the original features (which could not be provided due to confidentiality issues).

The response variable *Class* tell us whether a transaction was fraudulent (value = 1) or not (value = 0).

```{r fig.width=5, fig.height=3}
creditcard %>%
  ggplot(aes(x = Class)) +
    geom_bar(color = "grey", fill = "lightgrey") +
    theme_bw()
```

There are two additional features, *Time* (time in seconds between each transaction and the first transaction) and *Amount* (how much money was transferred in this transaction).

Because *Time* only tells us the order in which transactions have been done, it doesn't actually tell us anything about the actual times (i.e. time of day) of the transaction. Therefore, I am normalizing them by day and bin them into four groups according time of day.

```{r}
summary(creditcard$Time)

# how many seconds are 24 hours
# 1 hr = 60 mins = 60 x 60 s = 3600 s
3600 * 24

# separate transactions by day
creditcard$day <- ifelse(creditcard$Time > 3600 * 24, "day2", "day1")

# make transaction relative to day
creditcard$Time_day <- ifelse(creditcard$day == "day2", creditcard$Time - 86400, creditcard$Time)

summary(creditcard[creditcard$day == "day1", ]$Time_day)
summary(creditcard[creditcard$day == "day2", ]$Time_day)

# bin transactions according to time of day
creditcard$Time <- as.factor(ifelse(creditcard$Time_day <= 38138, "gr1", # mean 1st Qu.
                          ifelse(creditcard$Time_day <= 52327, "gr2", # mean mean
                                 ifelse(creditcard$Time_day <= 69580, "gr3", # mean 3rd Qu
                                        "gr4"))))
```

```{r fig.width=3, fig.height=1}
creditcard %>%
  ggplot(aes(x = day)) +
    geom_bar(color = "grey", fill = "lightgrey") +
    theme_bw()
```

```{r }
creditcard <- select(creditcard, -Time_day, -day)

# convert class variable to factor
creditcard$Class <- factor(creditcard$Class)
```

```{r fig.width=3, fig.height=1}
creditcard %>%
  ggplot(aes(x = Time)) +
    geom_bar(color = "grey", fill = "lightgrey") +
    theme_bw() +
    facet_wrap( ~ Class, scales = "free", ncol = 2)
```

```{r fig.width=3, fig.height=1}
summary(creditcard[creditcard$Class == "0", ]$Amount)
summary(creditcard[creditcard$Class == "1", ]$Amount)

creditcard %>%
  ggplot(aes(x = Amount)) +
    geom_histogram(color = "grey", fill = "lightgrey", bins = 50) +
    theme_bw() +
    facet_wrap( ~ Class, scales = "free", ncol = 2)
```

Interestingly, fraudulent creditcard transactions had a higher mean amount of money that was transferred, but the maximum amount was much lower compared to regular transactions.

<br>

## Modeling

For modeling, I am using R's H2O implementation with the *h2o* package. For more details and other examples, see my posts on [my machine learning webinar](https://shiring.github.io/machine_learning/2017/03/31/webinar_code), on [building neural nets with h2o](https://shiring.github.io/machine_learning/2017/02/27/h2o) and on [performing grid search for hyperparameter tuning](https://shiring.github.io/machine_learning/2017/03/07/grid_search).

```{r warning=FALSE, message=FALSE}
library(h2o)
h2o.init(nthreads = -1)

# convert data to H2OFrame
creditcard_hf <- as.h2o(creditcard)
```

Then, I am splitting the dataset into training and test sets. Because I want to check how a pre-trained model with perform, I am splitting my data into two separate training sets and one independent test set for final model comparison.

```{r tidy=FALSE}
splits <- h2o.splitFrame(creditcard_hf, 
                         ratios = c(0.4, 0.4), 
                         seed = 42)

train_unsupervised  <- splits[[1]]
train_supervised  <- splits[[2]]
test <- splits[[3]]

response <- "Class"
features <- setdiff(colnames(train_unsupervised), response)
```

<br>

### Autoencoders

First, I am training the unsupervised neural network model using deep learnign autoencoders. With *h2o*, we can simply set `autoencoder = TRUE`.

Here, I am applying a technique called "bottleneck" training, where the hidden layer in the middle is very small. This means that my model will have to reduce the dimensionality of the input data (in this case, down to 2 nodes/dimensions).

The autoencoder model will then learn the patterns of the input data irrespective of given class labels. Here, it will learn, which creditcard transactions are similar and which transactions are outliers or anomalies. We need to keep in mind though, that autoencoder models will be sensitive to outliers in our data, which might throw off otherwise typical patterns.

```{r}
model_nn <- h2o.deeplearning(x = features,
                             training_frame = train_unsupervised,
                             model_id = "model_nn",
                             autoencoder = TRUE,
                             reproducible = TRUE, #slow - turn off for real problems
                             ignore_const_cols = FALSE,
                             seed = 42,
                             hidden = c(10, 2, 10), 
                             epochs = 100,
                             activation = "Tanh")
```

Because training can take a while, I am saving the model:

```{r echo=TRUE, eval=FALSE}
h2o.saveModel(model_nn, path="model_nn", force = TRUE)
```

```{r}
model_nn <- h2o.loadModel("/Users/Shirin/Documents/Github/blog_posts_prep/wip/model_nn/model_nn")
model_nn
```

<br>

### Extracting hidden layers

Because I had used a bottleneck model with two nodes in the hidden layer in the middle, we can consider this dimensionality reduction to explore our feature space (similar to what to we could do with a principal component analysis).
We can extract this hidden feature with the `h2o.deepfeatures()` function and plot it to show the reduced representation of the input data.

```{r}
train_features <- h2o.deepfeatures(model_nn, train_unsupervised, layer = 2) %>%
  as.data.frame() %>%
  mutate(Class = as.vector(train_unsupervised[, 31]))

ggplot(train_features, aes(x = DF.L2.C1, y = DF.L2.C2, color = Class)) +
  geom_point(alpha = 0.1)
```

Here, we do not see a cluster of fraudulent transactions that is distinct from non-fraud instances, so dimensionality reduction with our autoencoder model alone is not sufficient to identify fraud in this dataset.

<br>

### Anomaly detection

Now, we can also ask which instances were considered outliers or anomalies within our test data, using the `h2o.anomaly()` function. Based on the autoencoder model that was trained before, the input data will be reconstructed and for each instance, the mean squarred error (MSE) between actual value and reconstruction is calculated.

I am also calculating the mean MSE for both class labels.

```{r}
anomaly <- h2o.anomaly(model_nn, test) %>%
  as.data.frame() %>%
  tibble::rownames_to_column() %>%
  mutate(Class = as.vector(test[, 31]))

mean_mse <- anomaly %>%
  group_by(Class) %>%
  summarise(mean = mean(Reconstruction.MSE))
```

This, we can now plot:

```{r}
ggplot(anomaly, aes(x = as.numeric(rowname), y = Reconstruction.MSE, color = as.factor(Class))) +
  geom_point(alpha = 0.3) +
  geom_hline(data = mean_mse, aes(yintercept = mean, color = Class)) +
  scale_color_brewer(palette = "Set1") +
  labs(x = "instance number",
       color = "Class")
```

As we can see in the plot, there is no perfect classification into fraud and non-fraud cases but the mean MSE is definitely higher for fraudulent transactions than for regular ones.

We can now identify outlier instances by sorting them according to MSE and apply a threshold for what we consider outliers. We could e.g. say that we consider every instance with a MSE > 0.02 (according to the plot above) to be an anomaly/outlier.

```{r}
outliers <- anomaly %>%
  arrange(-Reconstruction.MSE) %>%
  filter(Reconstruction.MSE > 0.02)

outliers %>%
  ggplot(aes(x = Class)) +
    geom_bar(color = "grey", fill = "lightgrey") +
    theme_bw()
```

As we can see from the plot above, outlier detection is not sufficient to correctly classify fraudulent creditcard transactions (at least not with this dataset).

<br>

### Supervised model with pre-trained features

We can now use the autoencoder model as a pretraining model for a supervised model. Here, I am again using a neural network. This model will now use the weights from the autoencoder model for model fitting.

```{r}
model_nn_2 <- h2o.deeplearning(y = response,
                               x = features,
                               training_frame = train_supervised,
                               pretrained_autoencoder  = "model_nn",
                               reproducible = TRUE, #slow - turn off for real problems
                               ignore_const_cols = FALSE,
                               seed = 42,
                               hidden = c(10, 2, 10), 
                               epochs = 100,
                               activation = "Tanh")
```

```{r echo=TRUE, eval=FALSE}
h2o.saveModel(model_nn_2, path="model_nn_2", force = TRUE)
```

```{r}
model_nn_2 <- h2o.loadModel("/Users/Shirin/Documents/Github/blog_posts_prep/wip/model_nn_2/DeepLearning_model_R_1493567001063_1")
model_nn_2
```

<br>

### Measuring model performance

Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.

Owing to such imbalance in data, an algorithm that does no feature analysis and predicts all the transactions as non-frauds will also achieve an accuracy of 99.828%. Hence, accuracy is not a correct measure of efficiency in our case. We need some other measure of correctness while classifying transactions as fraud or non-fraud.

Specificity vs Sensitivity curve: The aim of the plot is to view the cutoff probability at which the sum of the sensitivity(fraud detection accuracy) and specificity(non-fraud detection accuracy) of the model is maximum. This cutoff probability is threshold at which the model is classifying maximum fraud transactions as 'frauds' and non-fraud transactions as 'non-frauds'.

baseline accuracy â€“> 99.826785 %

consider doing precision/recall as well as sensitivity (recall)/specificity

```{r fig.width=6, fig.height=5, fig.align='center'}
perf <- h2o.performance(model_nn_2, test)
perf
```

```{r fig.width=6, fig.height=5, fig.align='center'}
as.data.frame(h2o.predict(object = model_nn_2, newdata = test))
```

<br>

```{r}
# Now train DRF on reduced feature space, first need to add response back
train_reduced <- h2o.cbind(train_reduced_x, train_supervised[,y])

rf1 <- h2o.randomForest(x = names(train_reduced_x), y = y, 
                        training_frame = train_reduced,
                        ntrees = 100, seed = 1)

#To evaluate the performance on the test set, we also need to project the test set into the reduced feature space.

test_reduced_x <- h2o.deepfeatures(ae_model, test, layer = 1)
test_reduced <- h2o.cbind(test_reduced_x, test[,y])

rf_perf <- h2o.performance(rf1, newdata = test_reduced)
h2o.mse(rf_perf)
```


Convert the test data into its autoencoded representation.

```{r}
# Note: Testing = Reconstructing the test dataset
test_autoenc <- h2o.predict(model_nn, test)
```

---

If you are interested in more machine learning posts, check out [the category listing for **machine_learning** on my blog](https://shiring.github.io/categories.html#machine_learning-ref).

---

```{r}
sessionInfo()
```

